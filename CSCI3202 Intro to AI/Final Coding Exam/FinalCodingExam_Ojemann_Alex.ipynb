{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI 3202, Fall 2021: Final Coding Exam\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "This practicum is due on Moodle by **11:59 PM on Sunday December 12**.  Your solutions to theoretical questions should be done in Markdown/LateX directly below the associated question. Your solutions to computational questions should include any relevant Python code, as well as results and any written commentary.\n",
    "\n",
    "**The rules:**\n",
    "\n",
    "1. Choose any TWO of the following three problems to submit. Note the problems that you are submitting below. If you do not choose two problems, the graders will grade the first 2 problems by default. The graders WILL NOT grade all three problems and pick the 2 highest scores. So it is your responsibility to clearly indicate the TWO problems you are choosing.\n",
    "\n",
    "**I choose Problem 1 and Problem 2**\n",
    "\n",
    "1. All work, code and analysis must be **your own**.\n",
    "1. You may use your course notes, posted lecture slides, textbook, in-class notebooks and homework solutions as resources.  You may also search online for answers to general knowledge questions, like the form of a probability distribution function, or how to perform a particular operation in Python.\n",
    "1. You may **not** post to message boards or other online resources asking for help.\n",
    "1. **You may not collaborate with classmates or anyone else.**\n",
    "1. This is meant to be like a coding portion of your final exam. So, I will be much less helpful than I typically am with homework. For example, I will not check answers, help debug your code, and so on.\n",
    "1. If you have a question, post it first as a **private** Piazza message. If I decide that it is appropriate for the entire class, then I will make it a public post (and anonymous).\n",
    "1. If something is left open-ended, it is probably because I intend for you to code it up however you want, and only care about the plots/analysis I see at the end. Feel free to ask clarifying questions though.\n",
    "\n",
    "Violation of these rules will result in an **F** and a trip to the Honor Code council.\n",
    "\n",
    "---\n",
    "**By writing your name below, you agree to abide by these rules:**\n",
    "\n",
    "**Your name:** Alex Ojemann\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# added packages\n",
    "import heapq\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "## [50 pts] Problem 1:  Route-finding\n",
    "\n",
    "Consider the map of the area to the west of the Engineering Center given below, with a fairly coarse Cartesian grid superimposed.\n",
    "\n",
    "<img src=\"http://www.cs.colorado.edu/~tonyewong/home/resources/engineering_center_grid_zoom.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "The green square at $(x,y)=(1,15)$ is the starting location, and you would like to walk from there to the yellow square at $(25,9)$ with the **shortest total path length**. The filled-in blue squares are obstacles, and you cannot walk through those locations.  You also cannot walk outside of this grid.\n",
    "\n",
    "Legal moves in the North/South/East/West directions have a step cost of 1. Moves in the diagonal direction (for example, from $(1,15)$ to $(2,14)$) are allowed, but they have a step cost of $\\sqrt{2}$. \n",
    "\n",
    "Of course, you can probably do this problem (and likely have to some degree, in your head) without a search algorithm. But that will hopefully provide a useful \"sanity check\" for your answer.\n",
    "\n",
    "#### Part A\n",
    "Write a function `adjacent_states(state)`:\n",
    "* takes a single argument `state`, which is a tuple representing a valid state in this state space\n",
    "* returns in some form the states reachable from `state` and the step costs. How exactly you do this is up to you. One possible format for what this function returns is a dictionary with the keys being the tuple locations and the values of the keys being the step costs. E.g: adjacent_states((1,1)) =  $\\{(2,1):1, (2,2):1.414\\}$\n",
    "\n",
    "Print to the screen the output for `adjacent_states((1,15))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(15, 1): 1, (14, 1): 1.4142135623730951}\n"
     ]
    }
   ],
   "source": [
    "# Your code here.\n",
    "#First, I'm going to create the array representing which spaces on the grid are buildings so I can test my methods\n",
    "#These are using my coordinate system where (0,0) is the top left corner not the coordinated system in the diagram\n",
    "occupiedStates=[]\n",
    "occupiedStates.append((2,0))\n",
    "occupiedStates.append((3,0))\n",
    "occupiedStates.append((4,0))\n",
    "occupiedStates.append((5,0))\n",
    "occupiedStates.append((6,0))\n",
    "occupiedStates.append((7,0))\n",
    "occupiedStates.append((8,0))\n",
    "occupiedStates.append((9,0))\n",
    "occupiedStates.append((10,0))\n",
    "occupiedStates.append((11,0))\n",
    "occupiedStates.append((12,0))\n",
    "occupiedStates.append((13,0))\n",
    "occupiedStates.append((14,0))\n",
    "occupiedStates.append((3,1))\n",
    "occupiedStates.append((4,1))\n",
    "occupiedStates.append((5,1))\n",
    "occupiedStates.append((6,1))\n",
    "occupiedStates.append((7,1))\n",
    "occupiedStates.append((8,1))\n",
    "occupiedStates.append((9,1))\n",
    "occupiedStates.append((10,1))\n",
    "occupiedStates.append((11,1))\n",
    "occupiedStates.append((12,1))\n",
    "occupiedStates.append((13,1))\n",
    "occupiedStates.append((4,2))\n",
    "occupiedStates.append((5,2))\n",
    "occupiedStates.append((6,2))\n",
    "occupiedStates.append((7,2))\n",
    "occupiedStates.append((8,2))\n",
    "occupiedStates.append((9,2))\n",
    "occupiedStates.append((10,2))\n",
    "occupiedStates.append((11,2))\n",
    "occupiedStates.append((12,2))\n",
    "occupiedStates.append((5,3))\n",
    "occupiedStates.append((6,3))\n",
    "occupiedStates.append((7,3))\n",
    "occupiedStates.append((8,3))\n",
    "occupiedStates.append((9,3))\n",
    "occupiedStates.append((10,3))\n",
    "occupiedStates.append((11,3))\n",
    "occupiedStates.append((4,9))\n",
    "occupiedStates.append((5,9))\n",
    "occupiedStates.append((6,9))\n",
    "occupiedStates.append((7,9))\n",
    "occupiedStates.append((2,10))\n",
    "occupiedStates.append((3,10))\n",
    "occupiedStates.append((4,10))\n",
    "occupiedStates.append((5,10))\n",
    "occupiedStates.append((6,10))\n",
    "occupiedStates.append((7,10))\n",
    "occupiedStates.append((2,11))\n",
    "occupiedStates.append((3,11))\n",
    "occupiedStates.append((4,11))\n",
    "occupiedStates.append((5,11))\n",
    "occupiedStates.append((6,11))\n",
    "occupiedStates.append((7,11))\n",
    "occupiedStates.append((2,12))\n",
    "occupiedStates.append((3,12))\n",
    "occupiedStates.append((4,12))\n",
    "occupiedStates.append((5,12))\n",
    "occupiedStates.append((6,12))\n",
    "occupiedStates.append((7,12))\n",
    "occupiedStates.append((2,13))\n",
    "occupiedStates.append((3,13))\n",
    "occupiedStates.append((4,13))\n",
    "occupiedStates.append((5,13))\n",
    "occupiedStates.append((0,20))\n",
    "occupiedStates.append((1,20))\n",
    "occupiedStates.append((2,20))\n",
    "occupiedStates.append((3,20))\n",
    "occupiedStates.append((4,20))\n",
    "occupiedStates.append((5,20))\n",
    "occupiedStates.append((0,21))\n",
    "occupiedStates.append((1,21))\n",
    "occupiedStates.append((2,21))\n",
    "occupiedStates.append((3,21))\n",
    "occupiedStates.append((4,21))\n",
    "occupiedStates.append((5,21))\n",
    "occupiedStates.append((0,22))\n",
    "occupiedStates.append((1,22))\n",
    "occupiedStates.append((2,22))\n",
    "occupiedStates.append((3,22))\n",
    "occupiedStates.append((4,22))\n",
    "occupiedStates.append((5,22))\n",
    "occupiedStates.append((0,23))\n",
    "occupiedStates.append((1,23))\n",
    "occupiedStates.append((2,23))\n",
    "occupiedStates.append((3,23))\n",
    "occupiedStates.append((4,23))\n",
    "occupiedStates.append((5,23))\n",
    "occupiedStates.append((0,24))\n",
    "occupiedStates.append((1,24))\n",
    "occupiedStates.append((2,24))\n",
    "occupiedStates.append((3,24))\n",
    "occupiedStates.append((4,24))\n",
    "occupiedStates.append((5,24))\n",
    "occupiedStates.append((15,4))\n",
    "occupiedStates.append((15,5))\n",
    "occupiedStates.append((15,6))\n",
    "occupiedStates.append((15,7))\n",
    "occupiedStates.append((15,8))\n",
    "occupiedStates.append((15,9))\n",
    "occupiedStates.append((15,10))\n",
    "occupiedStates.append((15,11))\n",
    "occupiedStates.append((15,12))\n",
    "occupiedStates.append((15,13))\n",
    "occupiedStates.append((15,14))\n",
    "occupiedStates.append((15,15))\n",
    "occupiedStates.append((15,16))\n",
    "occupiedStates.append((15,17))\n",
    "occupiedStates.append((15,18))\n",
    "occupiedStates.append((15,19))\n",
    "occupiedStates.append((15,20))\n",
    "occupiedStates.append((15,21))\n",
    "occupiedStates.append((15,22))\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def adjacent_states(state):\n",
    "    states={}\n",
    "    #Add all adjacent states if they're within the boundaries and unoccupied\n",
    "    if state[0]-1>=0 and (state[0]-1,state[1]) not in occupiedStates:\n",
    "        states.update({(state[0]-1,state[1]):1})\n",
    "    if state[0]+1<=15 and (state[0]+1,state[1]) not in occupiedStates:\n",
    "        states.update({(state[0]+1,state[1]):1})\n",
    "    if state[1]-1>=0 and (state[0],state[1]-1) not in occupiedStates:\n",
    "        states.update({(state[0],state[1]-1):1})\n",
    "    if state[0]+1<=24 and (state[0],state[1]+1) not in occupiedStates:\n",
    "        states.update({(state[0],state[1]+1):1})\n",
    "    if state[0]-1>=0 and state[1]-1>=0 and (state[0]-1,state[1]-1) not in occupiedStates:\n",
    "        states.update({(state[0]-1,state[1]-1):math.sqrt(2)})\n",
    "    if state[0]-1>=0 and state[1]+1<=24 and (state[0]-1,state[1]+1) not in occupiedStates:\n",
    "        states.update({(state[0]-1,state[1]+1):math.sqrt(2)})\n",
    "    if state[0]+1<=15 and state[1]-1>=0 and (state[0]+1,state[1]-1) not in occupiedStates:\n",
    "        states.update({(state[0]+1,state[1]-1):math.sqrt(2)})\n",
    "    if state[0]+1<=15 and state[1]+1<=24 and (state[0]+1,state[1]+1) not in occupiedStates:\n",
    "        states.update({(state[0]+1,state[1]+1):math.sqrt(2)})\n",
    "    return states\n",
    "    \n",
    "            \n",
    "print(adjacent_states((15,0))) #Equivalent to (1,1) in given coordinate system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "Three candidate heuristic functions might be:\n",
    "1. `heuristic_cols(state, goal)` = number of columns between the argument `state` and the `goal`\n",
    "1. `heuristic_rows(state, goal)` = number of rows between the argument `state` and the `goal`\n",
    "1. `heuristic_eucl(state, goal)` = Euclidean distance between the argument `state` and the `goal`\n",
    "\n",
    "Write a function `heuristic_max(state, goal)` that returns the maximum of all three of these heuristic functions for a given `state` and `goal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "6\n",
      "24.73863375370596\n"
     ]
    }
   ],
   "source": [
    "# Your code here.\n",
    "def heuristic_cols(state, goal):\n",
    "    return abs(goal[1]-state[1])\n",
    "def heuristic_rows(state, goal):\n",
    "    return abs(goal[0]-state[0])\n",
    "def heuristic_eucl(state, goal):\n",
    "    return math.sqrt(heuristic_cols(state, goal) ** 2 + heuristic_rows(state, goal) ** 2)\n",
    "print(heuristic_cols((1,0), (7,24)))\n",
    "print(heuristic_rows((1,0), (7,24)))\n",
    "print(heuristic_eucl((1,0), (7,24)))\n",
    "def heuristic_max(state, goal):\n",
    "    return max(heuristic_cols(state, goal),heuristic_rows(state, goal),heuristic_eucl(state, goal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "Is the Manhattan distance an admissible heuristic function for this problem?  Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No because it has the potential to overestimate the cost of getting from one state to another. For example, the lowest potential cost of getting from the start state to the end state depicted in the diagram is 26.485 while the Manhattan distance between those two states is 30."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "Use A\\* search and the `heuristic_max` heuristic to find the shortest path from the initial state at $(1,15)$ to the goal state at $(25,9)$. Your search **should not** build up the entire state space graph in memory. Instead, use the `adjacent_states` function from Part A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13), (2, 14), (3, 15), (3, 16), (4, 17), (4, 18), (5, 19), (6, 20), (6, 21), (6, 22), (6, 23), (7, 24)]\n"
     ]
    }
   ],
   "source": [
    "# Your code here. You may use our class lecture notebooks, your old homework, and/or the homework solutions \n",
    "# that are posted on our Canvas page.\n",
    "def path(previous, s, start): \n",
    "    '''\n",
    "    `previous` is a dictionary chaining together the predecessor state that led to each state\n",
    "    `s` will be None for the initial state\n",
    "    otherwise, start from the last state `s` and recursively trace `previous` back to the initial state,\n",
    "    constructing a list of states visited as we go\n",
    "    '''\n",
    "    if s == start:\n",
    "        return [start]\n",
    "    else:\n",
    "        return path(previous, previous[s],start)+[s]\n",
    "def astar_search(start, goal):\n",
    "    '''A* search from `start` to `goal`\n",
    "    start = initial state\n",
    "    goal = goal state\n",
    "    '''         \n",
    "    frontier = {}\n",
    "    for state in adjacent_states(start):\n",
    "        frontier.update({state:adjacent_states(start)[state]+heuristic_max(state,goal)})\n",
    "    explored = [start]\n",
    "    currentState=start\n",
    "    previous={}\n",
    "    while True:\n",
    "        for state in frontier:\n",
    "            if state == goal:\n",
    "                previous.update({goal:currentState})\n",
    "                return path(previous,goal,start)\n",
    "        best_cost=999999\n",
    "        best_option=currentState\n",
    "        for option in frontier:\n",
    "            if frontier[option] < best_cost:\n",
    "                best_cost = frontier[option]\n",
    "                best_option = option\n",
    "        explored.append(best_option)\n",
    "        previous.update({best_option:currentState})\n",
    "        currentState=best_option\n",
    "        for state in adjacent_states(currentState):\n",
    "            frontier.update({state:adjacent_states(currentState)[state]+heuristic_max(state,goal)})\n",
    "    \n",
    "print(astar_search((1,0),(7,24)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here.\n",
    "maze=[[0 for j in range(0,25)]for i in range(0,16)]\n",
    "for i in range(0,16):\n",
    "    for j in range(0,25):\n",
    "        if (i,j) in occupiedStates:\n",
    "            maze[15-i][j]=1\n",
    "def plot_path(path):\n",
    "    ''' visualize the binary `maze` (assumed numpy array) and solution `path` (if provided)'''\n",
    "\n",
    "    nrow=16\n",
    "    ncol=25\n",
    "    \n",
    "    # create colormap\n",
    "    cmap = colors.ListedColormap(['coral', 'slategray'])\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(maze, cmap=cmap, origin='lower')\n",
    "    \n",
    "    # draw gridlines\n",
    "    ax.grid(which='major', axis='both', linestyle='-', color='k')\n",
    "    ax.set_xticks(np.arange(-.5, ncol, 1))\n",
    "    ax.set_yticks(np.arange(-.5, nrow, 1))\n",
    "    ax.xaxis.set_ticklabels([])\n",
    "    ax.yaxis.set_ticklabels([])\n",
    "    \n",
    "    # now draw the solution path, if one is provided\n",
    "    if path:\n",
    "        for p in range(len(path)-1):\n",
    "            point = path[p]\n",
    "            nextpoint = path[p+1]\n",
    "            plt.plot([point[1],nextpoint[1]], [15-point[0],15-nextpoint[0]], c='black')\n",
    "        ax.set_title(\"A* Search Path\")\n",
    "    plt.show()\n",
    "plot_path(astar_search((1,0),(7,24)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "## [50 pts] Problem 2:  Reinforcement learning\n",
    "\n",
    "Consider a **cube** state space defined by $0 \\le x, y, z \\le L$. Suppose you are piloting/programming a drone to learn how to land on a platform at the center of the $z=0$ surface (the bottom). Some assumptions:\n",
    "* In this discrete world, if I say the drone is at $(x,y,z)$ I mean that it is in the box centered at $(x,y,z)$. And there are boxes (states) centered at $(x,y,z)$ for all $0 \\le x,y,z \\le L$. Each state is a 1 unit cube. So when $L=2$ (for example), there are cubes centered at each $x=0,1,2$, $y=0,1,2$ and so on, for a total state space size of $3^3 = 27$ states.\n",
    "* All of the states with $z=0$ are terminal states.\n",
    "* The state at the center of the bottom of the cubic state space is the landing pad. So, for example, when $L=4$, the landing pad is at $(x,y,z) = (2,2,0)$.\n",
    "* All terminal states ***except*** the landing pad have a reward of -1. The landing pad has a reward of +1.\n",
    "* All non-terminal states have a reward of -0.01.\n",
    "* The drone takes up exactly 1 cubic unit, and begins in a random non-terminal state.\n",
    "* The available actions in non-terminal states include moving exactly 1 unit Up (+z), Down (-z), North (+y), South (-y), East (+x) or West (-x). In a terminal state, the training episode should end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "Write a class `MDPLanding` to represent the Markov decision process for this drone. Include methods for:\n",
    "1. `actions(state)`, which should return a list of all actions available from the given state\n",
    "2. `reward(state)`, which should return the reward for the given state\n",
    "3. `result(state, action)`, which should return the resulting state of doing the given action in the given state\n",
    "\n",
    "and attributes for:\n",
    "1. `states`, which is just a list of all the states in the state space, where each state is represented as an $(x,y,z)$ tuple\n",
    "2. `terminal_states`, a dictionary where keys are the terminal state tuples and the values are the rewards associated with those terminal states\n",
    "3. `default_reward`, which is a scalar for the reward associated with non-terminal states\n",
    "4. `all_actions`, a list of all possible actions (Up, Down, North, South, East, West)\n",
    "5. `discount`, the discount factor (use $\\gamma = 0.999$ for this entire problem)\n",
    "\n",
    "How you feed arguments/information into the class constructor is up to you.\n",
    "\n",
    "Note that actions are *deterministic* here.  The drone does not need to learn transition probabilities for outcomes of particular actions. What the drone does need to learn, however, is where the heck that landing pad is, and how to get there from any initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "class MDPLanding:\n",
    "    def __init__(self, nrow, ncol, nplane, terminal, default_reward, discount):\n",
    "\n",
    "        \n",
    "        self.nrow = nrow\n",
    "        self.ncol = ncol\n",
    "        self.nplane=nplane\n",
    "        \n",
    "       # Using nrow and ncol, generate all of the tuples for blocks on the grid/room\n",
    "        self.states = [(x,y,z) for x in range(0,ncol+1) for y in range(0,nrow+1) for z in range(0,nplane+1)]\n",
    "        \n",
    "        self.terminal_states = terminal\n",
    "        self.default_reward = default_reward\n",
    "        self.df = discount\n",
    "        self.allActions=['N','S','E','W','U','D']\n",
    "\n",
    "    def actions(self, state):\n",
    "        \n",
    "        if state in self.terminal_states:\n",
    "            return [None]\n",
    "        else:\n",
    "            moves = []\n",
    "            if state[0] > 0:\n",
    "                # can move West\n",
    "                moves.append('W')\n",
    "            if state[0] < self.ncol:\n",
    "                # can move East\n",
    "                moves.append('E')\n",
    "            if state[1] > 0:\n",
    "                # can move South\n",
    "                moves.append('S')\n",
    "            if state[1] < self.nrow:\n",
    "                # can move North\n",
    "                moves.append('N')\n",
    "            if state[2] > 0:\n",
    "                # can move Down\n",
    "                moves.append('D')\n",
    "            if state[2] < self.nplane:\n",
    "                # can move Up\n",
    "                moves.append('U')\n",
    "            \n",
    "            return moves\n",
    "        \n",
    "    def reward(self, state):\n",
    "        return self.terminal_states[state] if state in self.terminal_states.keys() else self.default_reward\n",
    "        \n",
    "    def result(self, state, action):\n",
    "    \n",
    "        if action=='N':\n",
    "            new_state = (state[0], state[1]+1, state[2])\n",
    "        elif action=='S':\n",
    "            new_state = (state[0], state[1]-1, state[2])\n",
    "        elif action=='E':\n",
    "            new_state = (state[0]+1, state[1], state[2])\n",
    "        elif action=='W':\n",
    "            new_state = (state[0]-1, state[1], state[2])\n",
    "        elif action=='U':\n",
    "            new_state = (state[0], state[1], state[2]+1)\n",
    "        elif action=='D':\n",
    "            new_state = (state[0], state[1], state[2]-1)\n",
    "        \n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "Write a function to implement **policy iteration** for this drone landing MDP. Create an MDP environment to represent the $L=4$ case (so 125 total states).\n",
    "\n",
    "Use your function to find an optimal policy for your new MDP environment. Check (by printing to screen) that the policy for the following states are what you expect, and comment on the results:\n",
    "1. $(2,2,1)$\n",
    "1. $(0,2,1)$\n",
    "1. $(2,0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n",
      "E\n",
      "N\n"
     ]
    }
   ],
   "source": [
    "# Your code here.\n",
    "import random\n",
    "def policy_iteration(mdp,initialState):\n",
    "    policy=[]\n",
    "    #Start with 0 values random policy at all states\n",
    "    for i in range(0,mdp.ncol+1):\n",
    "        policy.append([])\n",
    "        for j in range(0,mdp.nrow+1):\n",
    "            policy[i].append([])\n",
    "            for k in range(0,mdp.nplane+1):\n",
    "                policy[i][j].append(random.choice(mdp.actions(initialState)))\n",
    "    values=[[[0 for x in range(0,mdp.ncol+1)] for y in range(0,mdp.nrow+1)] for z in range(0,mdp.nplane+1)]\n",
    "    count=0\n",
    "    while count<1000:\n",
    "        for i in range(0,mdp.ncol+1):\n",
    "            for j in range(0,mdp.nrow+1):\n",
    "                for k in range(0,mdp.nplane+1):\n",
    "                    currentOptions={}\n",
    "                    if (i,j,k) not in mdp.terminal_states:\n",
    "                        for action in mdp.actions((i,j,k)):\n",
    "                            currentOptions.update({action: mdp.reward(mdp.result((i,j,k),action))+mdp.df*values[mdp.result((i,j,k),action)[0]][mdp.result((i,j,k),action)[1]][mdp.result((i,j,k),action)[2]]})\n",
    "                        best_policy=''\n",
    "                        best_value=-999\n",
    "                        for option in currentOptions:\n",
    "                            if currentOptions[option]>best_value:\n",
    "                                best_value=currentOptions[option]\n",
    "                                best_policy=option\n",
    "                        values[i][j][k]=best_value\n",
    "                        policy[i][j][k]=best_policy\n",
    "                    else:\n",
    "                        values[i][j][k]=mdp.terminal_states[(i,j,k)]\n",
    "                        policy[i][j][k]='None'\n",
    "        count+=1\n",
    "    return policy\n",
    "terminalStates={}\n",
    "for i in range(0,5):\n",
    "    for j in range(0,5):\n",
    "        terminalStates.update({(i,j,0):-1})\n",
    "terminalStates[(2,2,0)]=1\n",
    "myMDP=MDPLanding(4,4,4,terminalStates,-0.01,0.999)\n",
    "m=policy_iteration(myMDP,(2,2,4))\n",
    "print(m[2][2][1])\n",
    "print(m[0][2][1])\n",
    "print(m[2][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "\n",
    "Code up a **Q-learning** agent/algorithm to learn how to land the drone. You can do this however you like, as long as you use the MDP class structure defined above.  \n",
    "\n",
    "Your code should include some kind of a wrapper to run many trials to train the agent and learn the Q values (see Section 22.3 in the textbook - page 803 might be of particular interest).  You also do not need to have a separate function for the actual \"agent\"; your code can just be a \"for\" loop within which you are refining your estimate of the Q values.\n",
    "\n",
    "From each training trial, save the cumulative discounted reward (utility) over the course of that episode. That is, add up all of $\\gamma^t R(s_t)$ where the drone is in state $s_t$ during time step $t$, for the entire sequence. I refer to this as \"cumulative reward\" because we usually refer to \"utility\" as the utility *under an optimal policy*.\n",
    "\n",
    "Some guidelines:\n",
    "* The drone should initialize in a random non-terminal state for each new training episode.\n",
    "* The training episodes should be limited to 50 time steps, even if the drone has not yet landed. If the drone lands (in a terminal state), the training episode is over.\n",
    "* You may use whatever learning rate $\\alpha$ you decide is appropriate, and gives good results.\n",
    "* There are many forms of Q-learning. You can use whatever you would like, subject to the reliability targets in Part D below.\n",
    "* Your code should return:\n",
    "  * The learned Q values associated with each state-action pair.\n",
    "  * The cumulative reward for each training trial. \n",
    "  * Anything else that might be useful in the ensuing analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "def qLearning(mdp, num_episodes, alpha = 0.6, epsilon = 0.1):\n",
    "    count=0\n",
    "    #randomly chosen best actions at each state before trials, similar to policy iteration\n",
    "    bestActions=[]\n",
    "    Q = {}\n",
    "    cumulativeUtilities=[]\n",
    "    for i in range(0,mdp.ncol+1):\n",
    "        bestActions.append([])\n",
    "        for j in range(0,mdp.nrow+1):\n",
    "            bestActions[i].append([])\n",
    "            for k in range(0,mdp.nplane+1):\n",
    "                bestActions[i][j].append(random.choice(mdp.actions((i,j,k))))\n",
    "    for state in mdp.states:\n",
    "        for action in mdp.actions(state):\n",
    "            Q.update({(state,action):0})\n",
    "    while count < num_episodes:\n",
    "        count2=0\n",
    "        #random start state for each trial\n",
    "        currentState=(random.randint(0,mdp.ncol),random.randint(0,mdp.nrow),random.randint(0,mdp.nplane))\n",
    "        cumulativeUtility=0\n",
    "        next_state=currentState\n",
    "        while count2 < 50:\n",
    "            cumulativeUtility+=mdp.reward(currentState)\n",
    "            if currentState in mdp.terminal_states:\n",
    "                break\n",
    "            if random.random() > epsilon:\n",
    "                next_state=mdp.result(currentState,bestActions[currentState[0]][currentState[1]][currentState[2]])\n",
    "            else:\n",
    "                next_state=mdp.result(currentState,random.choice(mdp.actions(currentState)))\n",
    "            count2+=1\n",
    "            best_next_action = ''\n",
    "            max_q=-999\n",
    "            for action in mdp.actions(next_state):\n",
    "                if Q[(next_state,action)]>max_q:\n",
    "                    max_q=Q[(next_state,action)]\n",
    "                    best_next_action=action\n",
    "            td_target = mdp.reward(next_state) + mdp.df * Q[(next_state,best_next_action)]\n",
    "            td_delta = td_target - Q[(currentState,bestActions[currentState[0]][currentState[1]][currentState[2]])]\n",
    "            Q[(currentState,bestActions[currentState[0]][currentState[1]][currentState[2]])] += alpha * td_delta\n",
    "            bestActions[next_state[0]][next_state[1]][next_state[2]]=best_next_action\n",
    "            currentState=next_state\n",
    "        cumulativeUtilities.append(cumulativeUtility)\n",
    "        count+=1\n",
    "    return (Q,bestActions,cumulativeUtilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "\n",
    "Initialize the $L=10$ environment (so that the landing pad is at $(5,5,0)$). Run some number of training trials to train the drone.\n",
    "\n",
    "**How do I know if my drone is learned enough?**  If you take the mean cumulative reward across the last 5000 training trials, it should be around 0.80. This means at least about 10,000 (but probably more) training episodes will be necessary. It will take a few seconds on your computer, so start small to test your codes.\n",
    "\n",
    "**Then:** Compute block means of cumulative reward from all of your training trials. Use blocks of 500 training trials. This means you need to create some kind of array-like structure such that its first element is the mean of the first 500 trials' cumulative rewards; its second element is the mean of the 501-1000th trials' cumulative rewards; and so on. Make a plot of the block mean rewards as the training progresses. It should increase from about -0.5 initially to somewhere around +0.8.\n",
    "\n",
    "**And:** Print to the screen the mean of the last 5000 trials' cumulative rewards, to verify that it is indeed about 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here.\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "terminalStates={}\n",
    "for i in range(0,11):\n",
    "    for j in range(0,11):\n",
    "        terminalStates.update({(i,j,0):-1})\n",
    "terminalStates[(5,5,0)]=1\n",
    "myNewMDP=MDPLanding(10,10,10,terminalStates,-0.01,0.999)\n",
    "array=qLearning(myNewMDP,50000)[2]\n",
    "arr1=[]\n",
    "arr2=[]\n",
    "arr3=[]\n",
    "arr4=[]\n",
    "arr5=[]\n",
    "arr6=[]\n",
    "arr7=[]\n",
    "arr8=[]\n",
    "arr9=[]\n",
    "arr10=[]\n",
    "for i in range(0,500):\n",
    "    arr1.append(array[i])\n",
    "for i in range(500,1000):\n",
    "    arr2.append(array[i])\n",
    "for i in range(1000,1500):\n",
    "    arr3.append(array[i])\n",
    "for i in range(1500,2000):\n",
    "    arr4.append(array[i])\n",
    "for i in range(2000,2500):\n",
    "    arr5.append(array[i])\n",
    "for i in range(2500,3000):\n",
    "    arr6.append(array[i])\n",
    "for i in range(3000,3500):\n",
    "    arr7.append(array[i])\n",
    "for i in range(3500,4000):\n",
    "    arr8.append(array[i])\n",
    "for i in range(4000,4500):\n",
    "    arr9.append(array[i])\n",
    "for i in range(4500,5000):\n",
    "    arr10.append(array[i])\n",
    "x=[]\n",
    "for i in range(0,10):\n",
    "    x.append(i)\n",
    "averages=[]\n",
    "averages.append(sum(arr1)/500)\n",
    "averages.append(sum(arr2)/500)\n",
    "averages.append(sum(arr3)/500)\n",
    "averages.append(sum(arr4)/500)\n",
    "averages.append(sum(arr5)/500)\n",
    "averages.append(sum(arr6)/500)\n",
    "averages.append(sum(arr7)/500)\n",
    "averages.append(sum(arr8)/500)\n",
    "averages.append(sum(arr9)/500)\n",
    "averages.append(sum(arr10)/500)\n",
    "plt.plot(x, averages, 'o', color='black');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part E\n",
    "\n",
    "**Question 1:** Why does the cumulative reward start off around -0.5 at the beginning of the training?\n",
    "\n",
    "**Question 2:** Why will it be difficult for us to train the drone to reliably obtain rewards much greater than about 0.8?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: Because the drone's best move is set to a random move at the beginning, it starts off not landing on the pad more often than not thus a negative cumulative utility is returned.\n",
    "\n",
    "2: This is because the drone is programmed to make a random move 10% (or whatever value you set epsilon to) of the time instead of the best move. The reason for this is so the drone is able to explore different areas so that it learns more about its environment but it does limit the accuracy of the drone once its had sufficient training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## [50 points] Problem 3:  Calibrating a model for global mean sea level changes\n",
    "\n",
    "<img src=\"http://www.anthropocenemagazine.org/wp-content/uploads/2017/05/future-sea-levels.jpg\" width=\"250\">\n",
    "\n",
    "\n",
    "\n",
    "**Part A:** Load and plot some data.\n",
    "\n",
    "Let's load a couple data sets.  `data_sealevel.csv` is a data set of global mean sea levels, and the other, `data_temperature.csv` is a data set of global mean temperatures. The following bullets discuss the quantities of interest. \n",
    "* `sealevel` will be a list of global mean sea levels (millimeters). This data is found in a column which resides within the `data_sealevel.csv`\n",
    "* `sealevel_sigma` will be a list of the *uncertainty* in global mean sea levels (millimeters). Use the column labeled `uncertainty` within the `data_sealevel.csv` file to obtain this data, and\n",
    "* `temperature` will be a list of global mean temperatures (degrees Celsius). This data is in the `temperature` column in the `data_temperature.csv` file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the suggested code to load in the data files. Feel free to modify these as you wish, but that\n",
    "# is not necessary.\n",
    "\n",
    "year = []\n",
    "sealevel = []\n",
    "sealevel_sigma = []\n",
    "temperature = []\n",
    "\n",
    "dfSealevel = pd.read_csv(\"data_sealevel.csv\")\n",
    "dfTemperature = pd.read_csv(\"data_temperature.csv\")\n",
    "\n",
    "# We aren't doing any heavy-duty stats stuff, so let's just keep what we need as regular lists\n",
    "year = dfSealevel[\"year\"].tolist()\n",
    "sealevel = dfSealevel[\"sealevel\"].tolist()\n",
    "sealevel_sigma = dfSealevel[\"uncertainty\"].tolist()\n",
    "temperature = dfTemperature[\"temperature\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A (i):**\n",
    "\n",
    "- Make three plots for Global mean surface temperature, Sea level (mm), and Sea Level Uncertainty (mm). The x-axis for each of these plots will be the years over which this data was collected. \n",
    "\n",
    "- Plot the data points as a scatter plots, and plot the three plots side-by-side-by-side (one row, three columns of figures). The point here is learn how to customize your figures a bit more, and also because computer screens are (typically) wider than they are tall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your plotting code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A (ii):** How does the uncertainty in global mean sea levels change as a function of time?  When is the uncertainty the highest?  Give one reason why you think this might be the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Part B:**  The \"out-of-box\" sea-level model\n",
    "\n",
    "In your plot from **(a)**, you should see quite an apparent relationship between increasing temperatures and rising sea levels.  Seeems like someone should try to model the relationship between those two, huh?\n",
    "\n",
    "In the helper function, slr, below, a simple model for temperature-driven changes in global mean sea level (GMSL) is defined. This is the model of [Rahmstorf (2007)](http://science.sciencemag.org/content/315/5810/368).\n",
    "\n",
    "The `slr` model takes two parameters, $\\alpha$ and $T_{eq}$, and requires a time series of global mean temperatures: `slr(alpha, Teq, temperature)`.\n",
    "* `alpha` is the sensitivity of sea-level changes to changes in global temperature. The units for $\\alpha$ are millimeters of sea-level changes per year, or mm y$^{-1}$.\n",
    "* `Teq` is the equilibrium global mean temperature, with units of degrees Celsius.\n",
    "* `temperature` is the time series of global mean surface temperatures, assumed to be relative to the 1961-1990 mean.\n",
    "\n",
    "For now, you do not need to worry too much about how this model works.  It is very simple, and widely used, but the point here is that you can plug in a particular set of temperatures (the model **forcing**) and parameters ($\\alpha$ and $T_{eq}$), and out pops a time series of simulated global mean sea levels.\n",
    "\n",
    "**Our goal:**  pick good values for $\\alpha$ and $T_{eq}$, so that when we run the `slr` model using the observations of temperature (which we plotted above), the model output matches well the observations of global mean sea level (which we also plotted above).\n",
    "\n",
    "The whole process of figuring out what these good parameter values are is called **model calibration**, and it's awesome.  Model Calibration is the point of this problem. Let's have a look at why we need to do this in the first place, shall we?\n",
    "\n",
    "The default parameter choices given in the Rahmstorf (2007) paper are $\\alpha=3.4$ mm y$^{-1}$ and $T_{eq} = -0.5\\ ^{\\circ}$C.\n",
    "\n",
    "**Your task for Part B:**\n",
    "\n",
    "Make a plot that contains:\n",
    "* the observed sea level data as scatter points\n",
    "* the modeled sea levels as a line, using the temperature observations from above as the `temperature` input\n",
    "* an appropriate legend and axis labels\n",
    "* $x$ axis is years\n",
    "* $y$ axis is sea level\n",
    "\n",
    "Note that after you run the `slr` model, you will need to **normalize** the output relative to the 1961-1990 reference period.  That is because you are going to compare it against data that is also normalized against this reference period. The `years` that correspond to the model output should be the same as the `years` that correspond to the `temperature` input. Normalizing data can mean several things. Follow the steps outlined below to \"normalize\" the data in the way needed for this problem:\n",
    "- Compute the mean of the output of the slr model for the years from 1961-1990 (inclusive).\n",
    "- Subtract this value from each entry in the \"sealevel\" list (list returned by the slr function)\n",
    "\n",
    "\n",
    "Make sure that you normalize the data prior to plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "\n",
    "def slr(alpha, Teq, temperature):\n",
    "    '''sea-level emulator of Rahmstorf 2007 (DOI: 10.1126/science.1135456)\n",
    "    Takes global mean temperature as forcing, and parameters:\n",
    "    alpha = temperature sensitivity of sea level rise, and\n",
    "    Teq   = equilibrium temperature,\n",
    "    and calculates a rise/fall in sea levels, based on whether the temperature\n",
    "    is warmer/cooler than the equilibrium temperature Teq.\n",
    "    Here, we are only worrying about alpha (for now!)'''\n",
    "\n",
    "    n_time = len(temperature)\n",
    "    deltat = 1\n",
    "    sealevel = [0]*n_time\n",
    "    sealevel[0] = -134\n",
    "    for t in range(n_time-1):\n",
    "        sealevel[t+1] = sealevel[t] + deltat*alpha*(temperature[t]-Teq)\n",
    "\n",
    "    return sealevel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your plot above ought to show decent match for the late 1900s, but diverge a bit further back in time.\n",
    "\n",
    "**The point:**  We can do better than this \"out-of-the-box\" version of the Rahmstorf sea level model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C:**   Figuring out our objective function\n",
    "\n",
    "As our **objective function**, we will use the joint likelihood function of the observed sea level data, given the model simulation.  The following is a detailed description of the derivation of the objective funciton for a hill climbing routine. **Note, you do not need to do anything for this part other than to read about the objective function and execute the cell below, then move to part D.**\n",
    "\n",
    "For a single data point in year $i$, $y_i$, with associated uncertainty $\\sigma_i$, we can assume the likelihood for our model simulation in year $i$, $\\eta_i$, follows a normal distribution centered at the data point.  The model simulation is a **deterministic** result of our parameter choices $\\alpha$ and $T_{eq}$, so we write the likelihood as:\n",
    "\n",
    "$$L(y_i \\mid \\alpha, T_{eq}) = \\dfrac{1}{\\sqrt{2 \\pi} \\sigma_i} e^{-\\dfrac{(\\eta_i(\\alpha, T_{eq}) - y_i)^2}{2\\sigma_i^2}}$$\n",
    "\n",
    "But that only uses a single data point.  Let's use all the data!  The **joint likelihood** is the product of all of the likelihoods associated with the individual data points. But that is the product of a lot of numbers that are less than 1, so it will be **tiny**.  Instead, we should try to optimize the **joint log-likelihood**, which is simply the (natural) logarithm of the joint likelihood function.\n",
    "\n",
    "If we assume the observational data ($y_i$) are all independent, then the joint log-likelihood is:\n",
    "\n",
    "$$l(\\mathbf{y} \\mid \\alpha, T_{eq}) = -\\dfrac{N}{2} \\log{(2\\pi)} - \\sum_{i=1}^N \\log{(\\sigma_i)} - \\dfrac{1}{2}\\sum_{i=1}^N \\left( \\dfrac{\\eta_i(\\alpha, T_{eq}) - y_i}{\\sigma_i} \\right)^2$$\n",
    "\n",
    "where, $\\mathbf{y} = [y_1, y_2, \\ldots, y_N]$ is the entire vector (list) of sea level observations, $\\eta(\\alpha, T_{eq}) = [\\eta_1(\\alpha, T_{eq}), \\eta_2(\\alpha, T_{eq}), \\ldots, \\eta_N(\\alpha, T_{eq})]$ is the entire vector (list) of `slr` model output when the parameter values $\\alpha$ and $T_{eq}$ are used, and $N$ is the number of observations we have.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining our objective function**\n",
    "\n",
    "Now define a `log_likelihood(parameters, obs_mu, obs_sigma)` function:\n",
    "* `parameters`: argument that is a list of two parameter values, $[\\alpha, T_{eq}]$\n",
    "  * within the likelihood function, you will need to generate the model simulation $\\eta(\\alpha, T_{eq})$ using the input `parameters`, for comparison against the observational data\n",
    "* `obs_temp`: argument that is a time series (list) of observed global mean temperatures, that will be used to run the `slr` model. Provide a default value of `temperature` for this, because we only have one temperature data set to use, and we don't want to keep \n",
    "* `obs_mu`: argument that is a time series (list) of observed values, that will be used for comparison against the `model` output. Provide a default value of `sealevel` here, because we won't be changing the observational data.\n",
    "* `obs_sigma`: argument that is a time series (list) of the corresponding uncertainties in the observational data. Simiarly, provide a default value of `sealevel_sigma` here, so we can avoid the tedious task of sending the data set into this function.\n",
    "* all three of these inputs should be lists, and should be the same length\n",
    "* this routine should return a **single** float number, that is the joint log-likelihood of the given `model` simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the objective function. You will be using this function below when you code up hill-climbing and \n",
    "# simulated annealing routines.\n",
    "\n",
    "def log_likelihood(parameters, obs_temp=temperature, obs_mu=sealevel, obs_sigma=sealevel_sigma):\n",
    "    model = slr(alpha=parameters[0], Teq=parameters[1], temperature=temperature)\n",
    "    \n",
    "    # normalize\n",
    "    reference = (year.index(1961), year.index(1990))\n",
    "    model -= np.mean(model[reference[0]:(reference[1]+1)])\n",
    "\n",
    "    return np.sum([np.log(stats.norm.pdf(x=model, loc=obs_mu, scale=obs_sigma))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D:**  Defining our class structure\n",
    "\n",
    "Now we will apply a hill-climbing algorithm to tune the $\\alpha$ and $T_{eq}$ parameters.\n",
    "\n",
    "Using our in-class lecture notebook on hill-climbing as a guide, do the following:\n",
    "\n",
    "* Define a `State` class, with attributes for the parameter values (which define the state) and the objective function value of that state.\n",
    "* Define a `Problem_hillclimb` **sub-class** of the more general class `Problem`, with:\n",
    "  * attributes for the current `State` (a `State` object), the `objective_function` (the log-likelihood defined above), and `stepsize`. You will need to play around to decide what an appropriate stepsize is. Keep in mind that you may need a different stepsize for each of $\\alpha$ and $T_{eq}$.\n",
    "  * methods for `moves` (return the list of all possible moves from the current state) and `best_move` (return the move that maximizes the objective function).\n",
    "  * the `moves` available should be in proper 2-dimensional space.  Do **not** simply optimize one parameter, keeping the other fixed, then optimize the other parameter, while keeping the first fixed.  (*That method *can* work, but there are some theoretical issues that would need to be tackled, and we are not getting into that here.*) You are allowed to restrict yourself to movements along a grid, as long as you entertain steps in both the $\\alpha$ and the $T_{eq}$ directions.\n",
    "* Define the `hill_climb` algorithm, with any necessary modifications (here, and in the above classes) for the new 2-dimensional state space.\n",
    "  * `hill_climb(problem, n_iter)`:  arguments are a `Problem_hillclimb` object and number of iterations, `n_iter`\n",
    "  * return a `State` that corresponds to the algorithm's guess at a global maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now:\n",
    "1. define an initial state object, using the default values from Rahmstorf 2007 as a starting point.\n",
    "2. define a hill-climbing problem object, using this initial state, the log-likelihood objective function, and stepsize(s) of your choosing. (The stepsize(s) may require some playing around to find something you are happy with.)\n",
    "3. ***hill-climb!!!*** Use a number of iterations that you deem appropriate. \n",
    "\n",
    "Play around until you have a simulation that you are happy with.  Then:\n",
    "1. Print to screen the parameter values and corresponding log-likelihood value.\n",
    "2. Compare this calibrated log-likelihood value to the \"out-of-box\" model (above).\n",
    "3. Make a plot of:\n",
    "  * the sea level observations as scatter points\n",
    "  * the uncalibrated model as one line\n",
    "  * the calibrated model as another line\n",
    "  * include axis labels and a legend\n",
    "  \n",
    "**\"Unit tests\":**\n",
    "* As a benchmark, make sure that your log-likelihood is *at least* -500.\n",
    "* Your calibrated (optimized) model simulation should be going straight through the data points.\n",
    "* If this isn't the case, remember to normalize your model against the 1961-1990 reference period!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part E:**  Simulated annealing\n",
    "\n",
    "Let's re-calibrate the `slr` model. This time, we will use **simulated annealing**. Again, using our in-class activity as a guide, do the following:\n",
    "\n",
    "* Continue to use your `State` class above.\n",
    "* Define a `Problem_annealing` sub-class of the `Problem` class, with:\n",
    "  * attributes for the current `State` (a `State` object), the `objective_function` (the log-likelihood defined above), and `stepsize`. You will need to play around to decide what an appropriate stepsize is. Keep in mind that you may need a different stepsize for each of $\\alpha$ and $T_{eq}$.\n",
    "  * method for `random_move`, to pick a random move **by drawing from a multivariate normal distribution**.  You should use the `stepsize` attribute as the covariance (width) for this.\n",
    "* Define the `simulated_annealing` algorithm, with any necessary modifications (here, and in the above classes) for the new 2-dimensional state space.\n",
    "  * `simulated_annealing(problem, n_iter)`:  arguments are a `Problem_annealing` object and number of iterations, `n_iter`\n",
    "  * return a `State` that corresponds to the algorithm's guess at a global maximum\n",
    "\n",
    "Subject to the above constraints, you may implement these however you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now:\n",
    "1. define an initial state object, using the default values from Rahmstorf 2007 as a starting point.\n",
    "2. define a simulated annealing problem object, using this initial state, the log-likelihood objective function, an appropriate temperature updating schedule and stepsize(s) of your choosing. (The stepsize(s) may require some playing around to find something you are happy with.)\n",
    "  * note that this \"temperature\" is distinct from the actual physical temperature used as input to drive the `slr` model\n",
    "3. ***anneal!!!*** Use a number of iterations that you deem appropriate. \n",
    "\n",
    "Play around until you have a simulation that you are happy with.  Then:\n",
    "1. Print to screen the parameter values and corresponding log-likelihood value.\n",
    "2. Compare this calibrated log-likelihood value to the \"out-of-box\" model (above).\n",
    "3. Make a plot of:\n",
    "  * the sea level observations as scatter points\n",
    "  * the uncalibrated model as one line\n",
    "  * the calibrated model as another line\n",
    "  * include axis labels and a legend\n",
    "  \n",
    "**\"Unit tests\":**  How does your model look when you plot it against the data? If it doesn't look good, then you failed this unit test :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part F:**\n",
    "\n",
    "Briefly summarize your findings. Specifically discuss the $\\alpha$ and $T_{eq}$ parameter values you found in **Part D** and **Part E**. How do these compare to the parameters of the model given by Rahmstorf? Did your hill-climbing and/or your simulated annealing programs find a better fit than the Rahmstorf model? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
