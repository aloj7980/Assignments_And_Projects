{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "This week we're going to do some simple exploration of the feature space of Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "B7z57Ageb0N0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 00:33:09.514423: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-04 00:33:09.623490: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# Building On Example from https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/\n",
    "# Building On Example from https://www.tensorflow.org/tutorials/images/cnn\n",
    "# Building On Example from https://www.guru99.com/rnn-tutorial.html\n",
    "# Building On Example from https://machinelearningmastery.com/prepare-univariate-time-series-data-long-short-term-memory-networks/\n",
    "# Building On Example from TensorFlow Documentation\n",
    "# For a multi-feature example, see: https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/23_Time-Series-Prediction.ipynb\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import sklearn.datasets\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSY4FLyaG_jG"
   },
   "source": [
    "First, let's set up a dataset that should be simple enough for us to solve, but complex enough for a variety of Neural Network tasks, including CNNs. We know digits already, and it's a good, usable set of images, so let's start there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-uOIWORtXLz",
    "outputId": "0085c091-1367-4b1d-fb4f-ce69781befb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64) (360, 64) [6 0 0 ... 2 7 1] [6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9\n",
      " 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5 5 4\n",
      " 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9 4 2 7\n",
      " 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9\n",
      " 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7 2 2 3 9 8 0 3\n",
      " 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8 6 0 4 5 2 7\n",
      " 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6 2 2 2 3 4 8 8\n",
      " 3 6 0 9 7 7 0 1 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9 9 8 5 3 3 2 0 5\n",
      " 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8 7 0 6 4 8 8 5 1 8\n",
      " 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4 3 8 3 5]\n"
     ]
    }
   ],
   "source": [
    "images = sklearn.datasets.load_digits()\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(images.data, images.target, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8Etj755HOV0"
   },
   "source": [
    "Next, let's make a Feed-Forward Neural Network. We're using the TensorFlow package[https://www.tensorflow.org/learn] to allow us to build these networks by components. See below for the description of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53Rl7kkB_gi1",
    "outputId": "b2c0bd18-050f-4383-903b-ca18e7e43de8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 16:16:05.357470: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 1ms/step - loss: 2.7659 - accuracy: 0.1761\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 979us/step - loss: 1.9368 - accuracy: 0.2804\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 933us/step - loss: 1.7105 - accuracy: 0.3535\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 923us/step - loss: 1.4320 - accuracy: 0.4906\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 922us/step - loss: 1.1489 - accuracy: 0.5797\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 888us/step - loss: 0.9310 - accuracy: 0.6729\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 927us/step - loss: 0.7894 - accuracy: 0.7223\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 942us/step - loss: 0.6817 - accuracy: 0.7794\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 921us/step - loss: 0.5809 - accuracy: 0.8051\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 913us/step - loss: 0.5088 - accuracy: 0.8323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f89306098a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layers_count = 1 #default 1\n",
    "hidden_nodes_count = 10 #default 10\n",
    "hidden_activations = 'relu' #default 'relu'\n",
    "epochs = 10 #default 10\n",
    "\n",
    "FFmodel = keras.Sequential()\n",
    "FFmodel.add(layers.Dense(hidden_nodes_count, input_shape = (64,), activation=hidden_activations)) #Need at least 1 hidden layer\n",
    "for i in range(hidden_layers_count - 1):\n",
    "        FFmodel.add(layers.Dense(hidden_nodes_count, activation=hidden_activations))\n",
    "FFmodel.add(layers.Dense(10, activation='softmax')) #Our output layer\n",
    "\n",
    "FFmodel.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "FFmodel.fit(X_train, y_train, epochs=epochs, batch_size=32) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbripjDfHXBs"
   },
   "source": [
    "\"Sequential\" is the object that allows us to add layers in sequence (hence the name) to build our model piece by piece.\n",
    "\n",
    "\"Dense\" is a fully connected layer of nodes. I set how many nodes I want in the layer and the activation function. In the starting layer, I also have to specify the size and shape of my input. In future layers, whatever is output from the previous layer is used as the input to this next layer.\n",
    "\n",
    "We compile a model to set up the metrics we will use in the fitting process as well as any evaluation metrics.\n",
    "\n",
    "We fit on the training set.\n",
    "\n",
    "### Q1: With the default values for number of (1) Hidden Layers, (10) Nodes per layer, ('relu') Activation, and (10) epochs, what do you get for Accuracy? \n",
    "#### Q1A: Run it a few times, at least three. What is your highest accuracy and your lowest accuracy after each set of 10 epochs?\n",
    "*Here you should be able to see whether this process is fully deterministic and always fits to the same results, or has some sort of randomness in it. Does this match your expectation for fitting a NN?*\n",
    "#### Q1B: What's the trend in your accuracy here?\n",
    "*Here it should match to your understanding of achieving convergence. Have we converged to an optimal set of internal weights yet?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Highest: .9074\n",
    "Lowest: .4342\n",
    "\n",
    "This matches my expectation because the process is not fully deterministic.\n",
    "\n",
    "b. The accuracy increases each epoch, which means we haven't converged to an optimal set of weights yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Z6I17-nI53F",
    "outputId": "09d61d3c-74c8-4650-96b4-5c77c5973237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0629 - accuracy: 0.9806\n",
      "Test Accuracy FFNN 0.980555534362793\n"
     ]
    }
   ],
   "source": [
    "loss, acc = FFmodel.evaluate(X_test, y_test)\n",
    "print('Test Accuracy FFNN ' + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: What is your test accuracy? How does it compare to your training accuracy?\n",
    "*This should be connecting to your understanding of training goodness and testing goodness, and the implications of underfitting and overfitting.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy is 0.767. This is better than the training accuracy of 0.752, so we can say we're not overfitting the training data, however it's possible we're underfitting because the training accuracy is usually a little higher than the test accuracy when the model is sufficiently fitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Tune your hyperparameters (number of hidden layers, number of nodes per layer, activation function, epochs) to achieve 100% Training Accuracy. Find *at least two combinations* of hyperparameters that achieve this objective. List the hyperparameter values and Test Accuracy results below, replacing the defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Q3 Responses Here*\n",
    "\n",
    "__FIRST FITTED MODEL__\n",
    " - hidden_layers_count = 1\n",
    " - hidden_nodes_count = 10000\n",
    " - hidden_activations = 'relu'\n",
    " - epochs = 25\n",
    " - Test Accuracy = 0.9806\n",
    "\n",
    "__SECOND FITTED MODEL__\n",
    " - hidden_layers_count = 1\n",
    " - hidden_nodes_count = 100\n",
    " - hidden_activations = 'elu'\n",
    " - epochs = 60 \n",
    " - Test Accuracy = 0.9806"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: What do your results above tell you about fitting a Feed-Forward Neural Network? For example...\n",
    "#### What relationships did you see between hyperparameters?\n",
    "#### What impacts did hyperparameters have on resulting overfitting or underfitting?\n",
    "#### Which hyperparameters made the model take longer to train?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the number of hidden layers decreased the accuracy, while increasing the number of epochs and hidden nodes increased the accuracy. The elu and linear values for the hidden activations parameter resulted in the highest accuracy. More epochs fundementally means more overfitting. Increasing the number epochs and the number of hidden nodes, especially once I tried values on the order of $10^6$, made the model take longer to train."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Fi4V2Z-IKiX"
   },
   "source": [
    "## BONUS 30 Points\n",
    "\n",
    "Now, let's try a Convolutional Neural Net! \n",
    "\n",
    "_hint: You will need to do the following steps:\n",
    " - Reshape your X_train and X_test samples into 2d arrays that are 8\\*8 using tensorflow's reshape.\n",
    " - add a 2D convolution layer for our input using the keras layer's Conv2D() object.\n",
    " - Add a 2D pooling layer for the result of our convolution using something like the keras layer's MaxPooling2D() object.\n",
    " - Flatten your layer so it is no longer a bunch of convolved, pooled 2D representations but instead is 1D to fit in a dense layer.\n",
    " - End the model in the same way as the FC network above.\n",
    "\n",
    "### Q 5: What hyperparameters did you use to get 100% accuracy on the training dataset?\n",
    " - Description of your 2D Convolution layer - Size of Convolution overlay? Stride Length? Activation?\n",
    " - Description of your Pooling layer - size?\n",
    " - Description of the number of Convolution + Pooling steps you added.\n",
    " - Description of the Dense layers you added after flattening.\n",
    " - Description of how many epochs it took to fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 01:35:25.174832: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 1s 8ms/step - loss: 2.0401 - accuracy: 0.4036 - val_loss: 0.6737 - val_accuracy: 0.8222\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.6023 - accuracy: 0.8135 - val_loss: 0.3011 - val_accuracy: 0.9111\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.3095 - accuracy: 0.9151 - val_loss: 0.1848 - val_accuracy: 0.9500\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.2283 - accuracy: 0.9374 - val_loss: 0.1481 - val_accuracy: 0.9611\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1826 - accuracy: 0.9478 - val_loss: 0.1301 - val_accuracy: 0.9583\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1351 - accuracy: 0.9645 - val_loss: 0.0936 - val_accuracy: 0.9750\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.1106 - accuracy: 0.9666 - val_loss: 0.0885 - val_accuracy: 0.9750\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0875 - accuracy: 0.9805 - val_loss: 0.0730 - val_accuracy: 0.9806\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0789 - accuracy: 0.9791 - val_loss: 0.0786 - val_accuracy: 0.9778\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 0s 4ms/step - loss: 0.0673 - accuracy: 0.9833 - val_loss: 0.0629 - val_accuracy: 0.9833\n"
     ]
    }
   ],
   "source": [
    "#A Cell for building a CNN model\n",
    "# Reshape input data\n",
    "X_train = tf.reshape(X_train, [-1, 8, 8, 1])\n",
    "X_test = tf.reshape(X_test, [-1, 8, 8, 1])\n",
    "\n",
    "# Build the model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(8, 8, 1)),\n",
    "    keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(units=10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel size of my 2d convolution layer is 3x3 with 'relu' activation. My pooling layer is of size 2x2. I added two dense layers with different activations after flattening the convolved and pooled matrix. I trained it for 10 epochs and the final accuracy was 0.9833. The accuracy was relatively stable after epoch 6 so I don't think more training epochs are necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
